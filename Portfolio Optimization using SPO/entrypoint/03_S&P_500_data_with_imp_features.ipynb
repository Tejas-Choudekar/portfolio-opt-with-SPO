{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3524d0",
   "metadata": {},
   "source": [
    "### Notebook summary\n",
    "In this notebook we have trained a model with only 6 important features selected by ```sklearn``` library's ```SelectKBest``` for AAPL ticker using SPO framework.\n",
    "Below points were observed:\n",
    "- Even with reduced features loss is in higher magnitude\n",
    "- Variance in loss is also high\n",
    "\n",
    "Next steps:\n",
    "- Reduce features based on domain knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988760c",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d64e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import math\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import random\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import yaml\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e1e43",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463be860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import data_exploration as de\n",
    "import model_training as mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70509813",
   "metadata": {},
   "source": [
    "### Load necessary directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a35f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path(os.getcwd())\n",
    "root_dir = current_dir\n",
    "while 'Portfolio Optimization using SPO' in root_dir.parts:\n",
    "    root_dir = root_dir.parent\n",
    "    if root_dir == Path(root_dir.root):\n",
    "        print(\"Root directory not found.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fe302",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = root_dir / \"Portfolio Optimization using SPO\" / \"config\" / \"config.yml\"\n",
    "complete_data_path = root_dir / \"Portfolio Optimization using SPO\" / \"data\" / \"dat_518_companies.csv\"\n",
    "data_path = root_dir / \"Portfolio Optimization using SPO\" / \"data\" / \"AAPL_df.csv\"\n",
    "cost_mat_path = root_dir / \"Portfolio Optimization using SPO\" / \"data\" / \"cost_mat.csv\"\n",
    "sigma_path = root_dir / \"Portfolio Optimization using SPO\" / \"data\" / \"sigma_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc22cb",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceeb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df_AAPL_train_test = pd.read_csv(data_path)\n",
    "df_final_returns = pd.read_csv(cost_mat_path)\n",
    "sigma_df = pd.read_csv(sigma_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = config[\"gamma\"]\n",
    "sigma = sigma_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32e754",
   "metadata": {},
   "source": [
    "### Calculate and plot feature importance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = df_AAPL_train_test.iloc[:,0:n_feats]\n",
    "target = df_AAPL_train_test.iloc[:,-1]\n",
    "all_features_fs, fs = mt.select_features(all_features, target, k='all')\n",
    "importance = list(fs.scores_)\n",
    "col_names = list(all_features.columns)\n",
    "fig = px.bar(x=col_names, y=importance).update_layout(title=\"Importance of each feature\", xaxis_title=\"Features\", yaxis_title=\"Mutual Importance Score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a59b3b",
   "metadata": {},
   "source": [
    "### Select top 6 features and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "six_features_fs, fs_6 = mt.select_features(all_features, target, k=6)\n",
    "mask = fs_6.get_support() #list of booleans\n",
    "new_features = [] # The list of your K best features\n",
    "\n",
    "for bool_val, feature in zip(mask, col_names):\n",
    "    if bool_val:\n",
    "        new_features.append(feature)\n",
    "        \n",
    "df_train_six_imp = pd.DataFrame(six_features_fs, columns=new_features)\n",
    "df_train_six_Y = pd.concat([df_train_six_imp, target], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ee31c",
   "metadata": {},
   "source": [
    "### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataframe\n",
    "df_AAPL_fi_train, df_AAPL_fi_test = train_test_split(df_train_six_Y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# cost vector\n",
    "df_final_returns_train, df_final_returns_test = train_test_split(df_final_returns, test_size=0.2, random_state=42, \n",
    "                                                                 shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124448b",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_n_rows, fi_n_cols = df_AAPL_fi_train.shape\n",
    "fi_n_feats = fi_n_cols-1\n",
    "\n",
    "# Instantiate the model\n",
    "model_fi_data = mt.get_model(n_feats = fi_n_feats)\n",
    "model_fi_data.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1779fb9",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "We will train the model with random hyper-parameters to test if everything is working fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trained_fi_model, epoch_fi_loss_list = mt.SGD_regressor(df_AAPL_fi_train, model_fi_data, df_final_returns_train, sigma, gamma, learning_rate= 0.0001, decay_rate=1.02, n_epochs=200, batch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255cfab4",
   "metadata": {},
   "source": [
    "### Plot loss progression with every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_fi = px.line(epoch_fi_loss_list).update_layout(title=\"Training Loss progression\", xaxis_title=\"epochs\", yaxis_title=\"SPO+ loss\")\n",
    "fig_fi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d8f32",
   "metadata": {},
   "source": [
    "### Testing the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdbd7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fi = trained_fi_model(df_AAPL_fi_test.iloc[:,0:fi_n_feats].values)\n",
    "fi_spo_test_loss = mt.get_SPO_plus_testing_loss(df_AAPL_fi_train, df_final_returns_test, y_pred_fi, sigma=sigma, gamma=gamma)\n",
    "\n",
    "print(f'The SPO+ loss on testing data is {fi_spo_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c24dde",
   "metadata": {},
   "source": [
    "After observing the loss at every epoch and also on testing data, the loss has very high magnitue and high variability as well so we will do feature selection based on domain knowledge from the research by [Zhong and Hitchcock (2021)](https://github.com/Shanlearning/SP-500-Stock-Prediction/tree/master) to see if the loss can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a0854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
