{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3524d0",
   "metadata": {},
   "source": [
    "### Notebook summary\n",
    "In this notebook we have trained a model with featurs by using domain knowledge from the research by [Zhong and Hitchcock (2021)](https://github.com/Shanlearning/SP-500-Stock-Prediction/tree/master) for AAPL ticker using SPO framework.\n",
    "Below points were observed:\n",
    "- Compared to previous experiments loss is reduced significantly\n",
    "- Hyper parameter tuning yielded good results\n",
    "- Compared with MAE, MSE and Huber loss where MSE and Huber loss performs better\n",
    "- Shapley interpretability hows clear distinction between SPO framework's decision making and decision making of other loss functions\n",
    "\n",
    "Next steps:\n",
    "- Improvement in scalability with deep learning\n",
    "- Rigorous testing of SPO framework on various cases in finance\n",
    "- Inclusion of unknown parameters in SPO framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988760c",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d64e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import math\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import random\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e1e43",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463be860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import data_exploration as de\n",
    "import model_training as mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70509813",
   "metadata": {},
   "source": [
    "### Load necessary directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a35f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path(os.getcwd())\n",
    "root_dir = current_dir\n",
    "while 'Portfolio Optimization using SPO' in root_dir.parts:\n",
    "    root_dir = root_dir.parent\n",
    "    if root_dir == Path(root_dir.root):\n",
    "        print(\"Root directory not found.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fe302",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = root_dir / \"Portfolio Optimization using SPO\" / \"config\" / \"config.yml\"\n",
    "complete_data_path = root_dir / \"Portfolio Optimization using SPO\" / \"data\" / \"dat_518_companies.csv\"\n",
    "data_path = root_dir / \"Portfolio Optimization using SPO\" / \"data\" / \"AAPL_df.csv\"\n",
    "cost_mat_path = root_dir / \"Portfolio Optimization using SPO\" / \"data\" / \"cost_mat.csv\"\n",
    "sigma_path = root_dir / \"Portfolio Optimization using SPO\" / \"data\" / \"sigma_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc22cb",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceeb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df_AAPL_train_test = pd.read_csv(data_path)\n",
    "df_final_returns = pd.read_csv(cost_mat_path)\n",
    "sigma_df = pd.read_csv(sigma_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = config[\"gamma\"]\n",
    "sigma = sigma_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32e754",
   "metadata": {},
   "source": [
    "### Selecting best features based on domain knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paper_cols = df_AAPL_train_test[config[\"best_feats\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ee31c",
   "metadata": {},
   "source": [
    "### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataframe\n",
    "df_paper_cols_train, df_paper_cols_test = train_test_split(df_paper_cols, test_size=0.2, random_state=42, \n",
    "                                                           shuffle=False)\n",
    "\n",
    "# cost vector\n",
    "df_final_returns_train, df_final_returns_test = train_test_split(df_final_returns, test_size=0.2, random_state=42, \n",
    "                                                                 shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124448b",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppr_n_rows, ppr_n_cols = df_paper_cols_train.shape\n",
    "ppr_n_feats = ppr_n_cols-1\n",
    "\n",
    "# Instantiate the model\n",
    "spo_model_ppr_data_2 = mt.get_model(n_feats = ppr_n_feats)\n",
    "spo_model_ppr_data_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0e5c4",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid Search to find best hyper-parameters\n",
    "\n",
    "# Parameters\n",
    "grid_params_df_paper = config[\"snp_grid_params\"]\n",
    "\n",
    "# Searching Best Parameters\n",
    "df_paper_results, df_paper_best_params, df_paper_error = mt.grid_search(df_paper_cols_train, df_final_returns_train, sigma=sigma, gamma=gamma, n_epoch=200, GridSearchParams=grid_params_df_paper)\n",
    "\n",
    "# Print results\n",
    "print(\"Results:\")\n",
    "for res in df_paper_results:\n",
    "    print(res)\n",
    "\n",
    "print(\"Best Parameters:\", df_paper_best_params)\n",
    "print(\"Error:\", df_paper_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1779fb9",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "We will train the model with best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trained_ppr_model_2, epoch_ppr_loss_list_2 = mt.SGD_regressor(df_paper_cols_train, spo_model_ppr_data_2, df_final_returns_train, sigma, gamma, learning_rate= 0.00001, decay_rate=2.02, n_epochs=200, batch_size = 512, decay = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255cfab4",
   "metadata": {},
   "source": [
    "### Plot loss progression with every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_ppr_spo = px.line(epoch_ppr_loss_list_2).update_layout(title=\"Training Loss progression\", xaxis_title=\"epochs\", yaxis_title=\"SPO+ loss\",legend={\n",
    "            \"title\": \"Loss Value\"})\n",
    "fig_ppr_spo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d8f32",
   "metadata": {},
   "source": [
    "### Testing the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdbd7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ppr = trained_ppr_model_2(df_paper_cols_test.iloc[:,0:ppr_n_feats].values)\n",
    "ppr_spo_test_loss = mt.get_SPO_plus_testing_loss(df_paper_cols_train, df_final_returns_test, y_pred_ppr, sigma=sigma, gamma=gamma)\n",
    "\n",
    "print(f'The SPO+ loss on testing data is {ppr_spo_test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a `.keras` zip archive.\n",
    "model_save_path = root_dir / \"Portfolio Optimization using SPO\" / \"models\" / \"trained_spo_model.keras\"\n",
    "trained_ppr_model_2.compile()\n",
    "trained_ppr_model_2.save(model_save_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada27e7",
   "metadata": {},
   "source": [
    "### Comparison with models trained on MAE, MSE and Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acdb9d",
   "metadata": {},
   "source": [
    "### Model trained on MAE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model_ppr_data_mae = mt.get_model(n_feats = ppr_n_feats, use_bias=False)\n",
    "model_ppr_data_mae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ppr_data_mae.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.00001),\n",
    "    loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_mae = model_ppr_data_mae.fit(\n",
    "    df_paper_cols_train[config[\"comp_vars\"]],\n",
    "    df_paper_cols_train[config[\"comp_target\"]],\n",
    "    epochs=200,\n",
    "    batch_size=512,\n",
    "    # Suppress logging.\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mae = tf.convert_to_tensor(model_ppr_data_mae.predict(df_paper_cols_test.iloc[:,0:ppr_n_feats].values))\n",
    "mae_spo_test_loss = mt.get_SPO_plus_testing_loss(df_paper_cols_train, df_final_returns_test, y_pred_mae, sigma=sigma, gamma=gamma)\n",
    "\n",
    "print(f'The SPO+ loss on testing data is {mae_spo_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca059f4",
   "metadata": {},
   "source": [
    "### Model trained on MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model_ppr_data_mse = mt.get_model(n_feats = ppr_n_feats, use_bias=False)\n",
    "model_ppr_data_mse.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ppr_data_mse.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.00001),\n",
    "    loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c7afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_mse = model_ppr_data_mse.fit(\n",
    "    df_paper_cols_train[config[\"comp_vars\"]],\n",
    "    df_paper_cols_train[config[\"comp_target\"]],\n",
    "    epochs=200,\n",
    "    batch_size=512,\n",
    "    # Suppress logging.\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mse = tf.convert_to_tensor(model_ppr_data_mse.predict(df_paper_cols_test.iloc[:,0:ppr_n_feats].values))\n",
    "mse_spo_test_loss = mt.get_SPO_plus_testing_loss(df_paper_cols_train, df_final_returns_test, y_pred_mse, sigma=sigma, gamma=gamma)\n",
    "\n",
    "print(f'The SPO+ loss on testing data is {mse_spo_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180da63f",
   "metadata": {},
   "source": [
    "### Model trained on Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d388a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model_ppr_data_huber = mt.get_model(n_feats = ppr_n_feats, use_bias=False)\n",
    "model_ppr_data_huber.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ppr_data_huber.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.00001),\n",
    "    loss='huber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_huber = model_ppr_data_huber.fit(\n",
    "    df_paper_cols_train[config[\"comp_vars\"]],\n",
    "    df_paper_cols_train[config[\"comp_target\"]],\n",
    "    epochs=200,\n",
    "    batch_size=512,\n",
    "    # Suppress logging.\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_huber = tf.convert_to_tensor(model_ppr_data_huber.predict(df_paper_cols_test.iloc[:,0:ppr_n_feats].values))\n",
    "huber_spo_test_loss = mt.get_SPO_plus_testing_loss(df_paper_cols_train, df_final_returns_test, y_pred_huber, sigma=sigma, gamma=gamma)\n",
    "\n",
    "print(f'The SPO+ loss on testing data is {huber_spo_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3400b3",
   "metadata": {},
   "source": [
    "### Shapley Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a38eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SPO model\n",
    "\n",
    "# DeepExplainer to explain predictions of the model\n",
    "explainer_spo = shap.DeepExplainer(trained_ppr_model_2, df_paper_cols_train.iloc[:,0:ppr_n_feats].values)\n",
    "# compute shap values\n",
    "shap_values_spo = explainer.shap_values(df_paper_cols_test.iloc[:,0:ppr_n_feats].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model trained on MAE\n",
    "\n",
    "# DeepExplainer to explain predictions of the model\n",
    "explainer_mae = shap.DeepExplainer(model_ppr_data_mae, df_paper_cols_train.iloc[:,0:ppr_n_feats].values)\n",
    "# compute shap values\n",
    "shap_values_mae = explainer_mae.shap_values(df_paper_cols_test.iloc[:,0:ppr_n_feats].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model trained on MSE\n",
    "\n",
    "# DeepExplainer to explain predictions of the model\n",
    "explainer_MSE = shap.DeepExplainer(model_ppr_data_mse, df_paper_cols_train.iloc[:,0:ppr_n_feats].values)\n",
    "# compute shap values\n",
    "shap_values_MSE = explainer_MSE.shap_values(df_paper_cols_test.iloc[:,0:ppr_n_feats].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model trained on Huber loss\n",
    "\n",
    "# DeepExplainer to explain predictions of the model\n",
    "explainer_huber = shap.DeepExplainer(model_ppr_data_huber, df_paper_cols_train.iloc[:,0:ppr_n_feats].values)\n",
    "# compute shap values\n",
    "shap_values_huber = explainer_huber.shap_values(df_paper_cols_test.iloc[:,0:ppr_n_feats].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b8b9a2",
   "metadata": {},
   "source": [
    "### Shap summary plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_spo[0], plot_type = 'bar', feature_names = df_paper_cols_test.iloc[:,0:ppr_n_feats].columns, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cc15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_mae[0], plot_type = 'bar', feature_names = df_paper_cols_test.iloc[:,0:ppr_n_feats].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_MSE[0], plot_type = 'bar', feature_names = df_paper_cols_test.iloc[:,0:ppr_n_feats].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ece12",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_huber[0], plot_type = 'bar', feature_names = df_paper_cols_test.iloc[:,0:ppr_n_feats].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d6a6b",
   "metadata": {},
   "source": [
    "### Shap waterfall plot for first observation on for model trained on SPO framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots._waterfall.waterfall_legacy(explainer_spo.expected_value[0].numpy(), shap_values_spo[0][0], feature_names = df_paper_cols_test.iloc[:,0:ppr_n_feats].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c24dde",
   "metadata": {},
   "source": [
    "After training the model on features selected using the research [Zhong and Hitchcock (2021)](https://github.com/Shanlearning/SP-500-Stock-Prediction/tree/master) The loss and variability is reduced sinificantly and the model seems to be converging. Comparing model trained on SPO framework with other models trained on MAE, MSE and Huber loss, we can see that the SPO model is almost as good as Huber and MSE model in terms of reducing the loss, but a clear distinction in the decision making can be seen from shap values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c0a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
